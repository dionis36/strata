# üìÑ ARTIFACT #4 ‚Äî Risk Scoring & Extraction Algorithm Specification


`/docs/RISK_AND_EXTRACTION_SPECIFICATION.md`

---

# 1Ô∏è‚É£ Core Design Philosophy

Risk and extraction priority must:

* Be reproducible
* Be explainable
* Be tunable
* Be statistically defensible
* Avoid arbitrary thresholds
* Avoid unbounded metrics

We therefore define:

* Clear normalization rules
* Clear weight justification
* Clear separation between structural risk and extraction suitability

These are NOT the same thing.

---

# 2Ô∏è‚É£ Definitions

Let:

* G = (V, E) be the directed weighted graph
* v ‚àà V be a component node
* N = total number of nodes

We define two independent models:

1. Structural Risk Model
2. Extraction Suitability Model

---

# 3Ô∏è‚É£ Structural Risk Model

Risk measures:

> ‚ÄúHow dangerous is this component to modify?‚Äù

It is NOT about modernization priority.

It is about systemic impact.

---

## 3.1 Raw Metrics

For each component node v:

* in_degree(v)
* out_degree(v)
* weighted_in_degree(v)
* weighted_out_degree(v)
* betweenness_centrality(v)
* loc(v)
* cyclomatic_complexity(v)
* db_write_count(v)
* db_read_count(v)

---

## 3.2 Normalization Strategy (CRITICAL)

All metrics must be normalized to [0,1].

We use Min-Max normalization:

```id="6qv3x2"
norm(x) = (x - min(X)) / (max(X) - min(X))
```

If max == min:

* norm(x) = 0

Why min-max instead of z-score?

Because:

* Graph metrics are skewed
* We need bounded values
* Risk score must stay within [0,1]

---

## 3.3 Structural Risk Formula

We define:

```id="y8mj7x"
R(v) =
  Œ± * norm(weighted_in_degree(v)) +
  Œ≤ * norm(weighted_out_degree(v)) +
  Œ≥ * norm(betweenness_centrality(v)) +
  Œ¥ * norm(cyclomatic_complexity(v)) +
  Œµ * DB_write_factor(v)
```

Where:

```id="xj2r91"
DB_write_factor(v) =
  min(1, db_write_count(v) / max_db_write_count)
```

---

## 3.4 Weight Justification

Recommended starting weights:

```id="p9z4mf"
Œ± = 0.30   (incoming coupling)
Œ≤ = 0.15   (outgoing dependencies)
Œ≥ = 0.20   (bridge role)
Œ¥ = 0.20   (complexity)
Œµ = 0.15   (write risk)
```

Why incoming weighted highest?

Because:

* If many depend on it ‚Üí modifying it breaks system

Why betweenness?

Because:

* Bridge nodes amplify risk beyond local degree

---

## 3.5 Risk Classification

Instead of fixed thresholds, use percentile bands:

* Top 20% ‚Üí High Risk
* Middle 50% ‚Üí Medium Risk
* Bottom 30% ‚Üí Low Risk

Why percentile instead of fixed values?

Because graph distributions vary per project.

This makes your model adaptive.

---

# 4Ô∏è‚É£ Extraction Suitability Model

Extraction ‚â† Low Risk.

Extraction asks:

> ‚ÄúIs this component suitable to be isolated into a service/module?‚Äù

---

## 4.1 Extraction Principles

A good extraction candidate:

* Has low incoming coupling
* Has cohesive internal logic
* Has limited DB table spread
* Has manageable size
* Does not act as a bridge node

---

## 4.2 Derived Metrics

For node v:

* normalized_in_degree(v)
* blast_radius_score(v)
* db_table_spread(v)
* loc(v)
* cohesion_proxy(v)

---

## 4.3 Cohesion Proxy

We approximate cohesion as:

```id="3nq5b1"
cohesion_proxy(v) =
  1 / (1 + normalized_out_degree(v))
```

Lower outward dependency ‚Üí higher cohesion.

---

## 4.4 DB Isolation Score

Let:

```id="5yvb4p"
db_table_spread(v) = 
  number_of_unique_tables(v) / total_tables
```

Isolation score:

```id="t7mk29"
db_isolation_score(v) = 1 - norm(db_table_spread(v))
```

Fewer tables = better isolation.

---

## 4.5 Blast Radius Score

Defined earlier:

```id="km9f2a"
blast_radius(v) =
  reachable_nodes(v) / total_nodes
```

Normalize.

---

## 4.6 Extraction Suitability Formula

```id="b2fw1d"
E(v) =
  Œ±1 * (1 - norm(in_degree(v))) +
  Œ±2 * cohesion_proxy(v) +
  Œ±3 * (1 - blast_radius(v)) +
  Œ±4 * db_isolation_score(v) +
  Œ±5 * size_factor(v)
```

Where:

```id="vmd8t4"
size_factor(v) =
  1 - norm(loc(v))
```

Smaller modules are easier to extract.

---

## 4.7 Suggested Weights

```id="hj8k3m"
Œ±1 = 0.30
Œ±2 = 0.20
Œ±3 = 0.20
Œ±4 = 0.20
Œ±5 = 0.10
```

Incoming coupling matters most.

---

# 5Ô∏è‚É£ Preventing False Positives

Add constraints:

If:

* R(v) > 0.80 (very high risk)
* AND blast_radius(v) > 0.50

Then:

Mark as:

> ‚ÄúCore Node ‚Äî Not Recommended for Early Extraction‚Äù

This prevents naive modernization attempts.

---

# 6Ô∏è‚É£ Implementation Pipeline

Order matters.

1. Build graph
2. Compute degrees
3. Compute centrality
4. Compute blast radius for all nodes
5. Normalize metrics
6. Compute R(v)
7. Compute E(v)
8. Assign percentile categories
9. Sort extraction candidates by E(v)

Never compute extraction before centrality.

---

# 7Ô∏è‚É£ Validation Strategy

To validate model quality:

### 7.1 Ground Truth Dataset

Manually label:

* 10 high-risk components
* 10 low-risk components
* 10 good extraction candidates

From a real open-source PHP project.

---

### 7.2 Evaluation Metrics

For risk classification:

* Precision
* Recall
* F1 score

For extraction ranking:

* Top-K accuracy (K = 5 or 10)

---

# 8Ô∏è‚É£ Sensitivity Analysis (Academic Strength)

You must demonstrate:

* Change Œ± weights by ¬±10%
* Observe ranking stability
* Show robustness

If rankings fluctuate wildly ‚Üí model unstable.

Document this.

---

# 9Ô∏è‚É£ Computational Complexity

Let:

* V = number of nodes
* E = number of edges

Complexity:

* Degree calculation: O(E)
* Centrality (betweenness): O(VE)
* Blast radius per node: O(V + E)
* Total worst-case: O(V(V + E))

For large graphs:

* Cache results
* Possibly approximate centrality

---

# 10Ô∏è‚É£ Critical Weaknesses

Be honest:

* Cohesion proxy is structural, not semantic
* SQL parsing heuristic limits DB accuracy
* Static analysis misses runtime resolution
* Centrality may overvalue utility classes
* Extraction suitability is probabilistic, not guaranteed

These go in your thesis limitations section.

---

# 11Ô∏è‚É£ Why This Specification Is Strong

It is:

* Quantitative
* Reproducible
* Adaptive
* Percentile-based
* Weight-justified
* Evaluation-ready
* Complexity-aware
* Defensive against misuse

It transforms your tool into a decision-support system rather than a visual analyzer.
