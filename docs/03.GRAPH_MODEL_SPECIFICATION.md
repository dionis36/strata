# üìÑ ARTIFACT #3 ‚Äî Graph Model Specification

`/docs/GRAPH_MODEL_SPECIFICATION.md`

---

# 1Ô∏è‚É£ Purpose of the Graph Model

The graph represents structural coupling in a legacy PHP system.

It must support:

1. Dependency visualization
2. Centrality calculation
3. Blast radius simulation
4. Extraction candidate ranking
5. DB hotspot detection
6. Strangler Fig phase grouping

This is not just a visualization graph.

It is an analytical graph.

---

# 2Ô∏è‚É£ Graph Type Definition

We define the graph formally as:

```
G = (V, E, W, T)
```

Where:

* V = set of nodes
* E = set of directed edges
* W = edge weights
* T = edge types

This is a:

> Directed, typed, weighted multigraph

We allow multiple edge types between the same nodes.

Implementation choice:

* NetworkX DiGraph (MVP)
* Upgrade to MultiDiGraph if needed

---

# 3Ô∏è‚É£ Node Model

Nodes represent structural units extracted from AST.

We define three node types:

---

## üîπ 3.1 File Node

Represents a PHP file.

Attributes:

| Attribute       | Source        | Purpose         |
| --------------- | ------------- | --------------- |
| node_id         | hash(path)    | Stable ID       |
| type            | "file"        | Graph filtering |
| path            | AST file.path | Traceability    |
| loc             | computed      | Effort estimate |
| db_access_count | derived       | Hotspot         |
| risk_score      | computed      | Ranking         |
| centrality      | computed      | Coupling metric |

---

## üîπ 3.2 Component Node

Represents class, interface, trait, or function.

Attributes:

| Attribute            | Source                         |
| -------------------- | ------------------------------ |
| node_id              | component_id                   |
| type                 | class/function/interface/trait |
| fully_qualified_name | AST                            |
| file_node_id         | parent                         |
| loc                  | AST metrics                    |
| complexity           | AST metrics                    |
| has_db_access        | derived                        |
| db_tables            | aggregated                     |
| risk_score           | computed                       |

Component nodes are primary candidates for extraction.

---

## üîπ 3.3 Database Table Node

Optional but strongly recommended.

Represents detected DB tables.

Why include them?

Because:

* You can measure coupling to specific tables
* You can detect table hotspots
* You can group extraction candidates by DB boundary

Attributes:

| Attribute    | Source           |
| ------------ | ---------------- |
| node_id      | hash(table_name) |
| type         | "db_table"       |
| table_name   | parsed           |
| access_count | derived          |

---

# 4Ô∏è‚É£ Edge Taxonomy (CRITICAL)

Edges must be typed.

Edge types:

---

## üîπ Structural Edges

| Type         | Meaning           |
| ------------ | ----------------- |
| calls        | A calls B         |
| instantiates | A creates B       |
| extends      | Class inheritance |
| implements   | Interface binding |
| uses_trait   | Trait inclusion   |

---

## üîπ File-Level Edges

| Type     | Meaning         |
| -------- | --------------- |
| includes | require/include |
| requires | require_once    |

---

## üîπ Data Access Edges

| Type         | Meaning                |
| ------------ | ---------------------- |
| reads_table  | Component reads table  |
| writes_table | Component writes table |

---

# 5Ô∏è‚É£ Edge Weighting Model

Weights influence risk and centrality.

We define base weights:

| Edge Type    | Weight |
| ------------ | ------ |
| calls        | 1.0    |
| instantiates | 1.2    |
| extends      | 2.0    |
| implements   | 1.8    |
| uses_trait   | 1.5    |
| includes     | 1.0    |
| reads_table  | 2.0    |
| writes_table | 3.0    |

Why higher weight for writes?

Because write operations increase system coupling risk.

---

Edge weight formula:

```
W = base_weight √ó frequency_multiplier
```

Where:

frequency_multiplier =

```
1 + log(call_count + 1)
```

This prevents frequency from exploding linearly.

---

# 6Ô∏è‚É£ Graph Construction Algorithm

For each file:

1. Create file node
2. For each component:

   * Create component node
   * Add edge file ‚Üí component (contains)
3. For each dependency:

   * Resolve FQN if possible
   * Add directed edge
4. For each DB access:

   * Create table node if missing
   * Add reads_table or writes_table edge

Unresolved references:

* Create placeholder nodes
* Mark as external_dependency = True

---

# 7Ô∏è‚É£ Risk Scoring Model (Graph-Based)

Risk must not be arbitrary.

Define:

```
risk_score = 
  Œ± * normalized_in_degree +
  Œ≤ * normalized_out_degree +
  Œ≥ * normalized_complexity +
  Œ¥ * db_write_factor +
  Œµ * centrality_score
```

Where recommended initial weights:

```
Œ± = 0.25
Œ≤ = 0.20
Œ≥ = 0.20
Œ¥ = 0.20
Œµ = 0.15
```

Normalize values between 0 and 1.

---

## Risk Level Thresholds

| Score       | Level  |
| ----------- | ------ |
| 0.00 ‚Äì 0.33 | Low    |
| 0.34 ‚Äì 0.66 | Medium |
| 0.67 ‚Äì 1.00 | High   |

---

# 8Ô∏è‚É£ Blast Radius Simulation Model

Definition:

When node N is removed:

Blast Radius = all nodes reachable from N via outgoing edges.

Algorithm:

1. Remove node
2. Perform DFS/BFS
3. Count reachable nodes
4. Compute percentage of graph impacted

Impact Score:

```
impact_ratio = affected_nodes / total_nodes
```

Classify:

| Impact Ratio | Severity |
| ------------ | -------- |
| <10%         | Low      |
| 10‚Äì30%       | Moderate |
| >30%         | Critical |

---

# 9Ô∏è‚É£ Extraction Candidate Ranking Model

Extraction priority must balance:

* Low incoming edges
* High cohesion
* High DB isolation
* Moderate size

Define:

```
extraction_score =
  (1 - normalized_in_degree) * 0.4 +
  db_isolation_score * 0.3 +
  (1 - blast_radius_score) * 0.2 +
  size_penalty_factor * 0.1
```

Higher score = better candidate.

---

# 10Ô∏è‚É£ Graph Serialization Format

Use NetworkX node_link_data:

```json id="d1k1h1"
{
  "directed": true,
  "multigraph": false,
  "nodes": [
    {
      "id": "node_id",
      "type": "component",
      "risk_score": 0.72
    }
  ],
  "links": [
    {
      "source": "node_id_A",
      "target": "node_id_B",
      "type": "calls",
      "weight": 1.2
    }
  ]
}
```

This format:

* Is JSON-safe
* Easily rehydrated
* UI-friendly

---

# 11Ô∏è‚É£ Graph Integrity Rules

Must enforce:

* No self-loops unless explicitly valid
* No duplicate edges
* All nodes must exist before edge insertion
* Placeholder nodes marked explicitly
* Graph must be acyclic for inheritance edges (validate)

---

# 12Ô∏è‚É£ Performance Considerations

For 1000+ files:

* Use adjacency list storage
* Avoid recomputing centrality repeatedly
* Cache degree metrics
* Compute heavy metrics lazily

---

# 13Ô∏è‚É£ Known Limitations

* Cannot detect runtime polymorphism
* Cannot resolve dynamic class names
* Cross-file name resolution depends on namespace accuracy
* Centrality is structural, not semantic

---

# 14Ô∏è‚É£ Why This Graph Model Is Strong

It is:

* Typed
* Weighted
* Quantitative
* Simulation-ready
* DB-aware
* Extraction-aware
* Evaluation-ready

It transforms your tool from a visualizer into an analytical engine.


