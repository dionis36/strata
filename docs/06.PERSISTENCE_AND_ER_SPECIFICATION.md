# üìÑ ARTIFACT #6 ‚Äî Persistence & ER Design Specification

Save as:
`/docs/PERSISTENCE_AND_ER_SPECIFICATION.md`

---

# 1Ô∏è‚É£ Strategic Design Decision

You have three options:

1. Pure JSON artifacts
2. Pure relational database
3. Hybrid model

---

## ‚ùå Pure JSON (Weak)

Pros:

* Simple
* Easy export

Cons:

* Hard to query across runs
* Hard to compute evaluation metrics
* Hard to compare two analyses
* No indexing
* No integrity guarantees

Reject as sole storage.

---

## ‚ùå Pure Relational DB (Overkill for MVP)

Pros:

* Queryable
* Structured

Cons:

* Hard to export graph easily
* Harder to debug
* Harder for collaborators to inspect

---

## ‚úÖ Hybrid Model (Recommended)

* Canonical graph.json stored per analysis run
* Canonical summary.json stored per analysis run
* SQLite database tracks:

  * Projects
  * Analysis runs
  * Node metrics
  * Extraction ranking
  * Evaluation labels

This gives:

* Reproducibility
* Queryability
* Easy export
* Academic rigor

---

# 2Ô∏è‚É£ Conceptual Data Model

Entities:

* Project
* AnalysisRun
* File
* Component
* Edge
* TableNode
* RiskMetric
* ExtractionCandidate
* EvaluationLabel

---

# 3Ô∏è‚É£ ER Model (Logical Design)

We define relationships formally.

---

## üîπ 3.1 Project

Represents a target codebase.

Attributes:

| Field      | Type      | Purpose            |
| ---------- | --------- | ------------------ |
| id         | PK        | internal reference |
| name       | text      | user label         |
| root_path  | text      | mounted path       |
| created_at | timestamp | traceability       |
| last_hash  | text      | change detection   |

One project ‚Üí many analysis runs.

---

## üîπ 3.2 AnalysisRun

Represents one full analysis execution.

| Field        | Type      |
| ------------ | --------- |
| id           | PK        |
| project_id   | FK        |
| started_at   | timestamp |
| completed_at | timestamp |
| status       | enum      |
| total_files  | int       |
| total_nodes  | int       |
| total_edges  | int       |
| graph_path   | text      |
| summary_path | text      |

Critical:

You must store artifact paths ‚Äî not embed graph in DB.

---

## üîπ 3.3 File

Represents analyzed file snapshot per run.

Composite uniqueness:

(run_id, path)

Fields:

| Field       | Type |
| ----------- | ---- |
| id          | PK   |
| run_id      | FK   |
| path        | text |
| hash_sha256 | text |
| size_bytes  | int  |
| loc         | int  |

Reason:

Files can change between runs.

---

## üîπ 3.4 Component

Represents graph node of type component.

Fields:

| Field                | Type               |
| -------------------- | ------------------ |
| id                   | PK                 |
| run_id               | FK                 |
| file_id              | FK                 |
| component_id         | text (stable hash) |
| name                 | text               |
| type                 | text               |
| fully_qualified_name | text               |
| loc                  | int                |
| complexity           | int                |
| in_degree            | int                |
| out_degree           | int                |
| weighted_in_degree   | float              |
| weighted_out_degree  | float              |
| centrality           | float              |
| blast_radius         | float              |
| risk_score           | float              |
| risk_level           | text               |
| extraction_score     | float              |
| extraction_rank      | int                |

Index:

* run_id
* risk_score
* extraction_score

---

## üîπ 3.5 Edge

You do NOT need to store every edge in DB unless:

* You plan complex DB queries

Recommendation:

Store edges only in graph.json.

But if storing:

| Field               |
| ------------------- |
| run_id              |
| source_component_id |
| target_component_id |
| type                |
| weight              |

Index on (run_id, source)

---

## üîπ 3.6 TableNode

Represents DB table.

| Field        |
| ------------ |
| id           |
| run_id       |
| table_name   |
| access_count |
| write_count  |

---

## üîπ 3.7 EvaluationLabel (CRITICAL FOR ACADEMICS)

For manual ground truth validation.

| Field                       |
| --------------------------- |
| id                          |
| run_id                      |
| component_id                |
| is_high_risk_expected       |
| is_good_extraction_expected |

This enables precision/recall computation.

Without this table, your evaluation section is weak.

---

# 4Ô∏è‚É£ ER Relationships

Formally:

Project (1) ‚Äî (N) AnalysisRun
AnalysisRun (1) ‚Äî (N) File
File (1) ‚Äî (N) Component
AnalysisRun (1) ‚Äî (N) TableNode
Component (N) ‚Äî (N) TableNode (via implicit graph edges)

EvaluationLabel (N) ‚Äî (1) Component

---

# 5Ô∏è‚É£ Integrity Constraints

You must enforce:

* On delete run ‚Üí cascade components
* component_id must be unique per run
* file hash must be stored
* extraction_rank must be unique per run

---

# 6Ô∏è‚É£ Index Strategy (Critical for Performance)

Indexes:

* idx_component_run
* idx_component_risk
* idx_component_extraction
* idx_file_hash
* idx_evaluation_component

Without indexes:
Large projects become slow.

---

# 7Ô∏è‚É£ Caching Strategy

When starting analysis:

Compute project-wide hash:

```id="k8v39s"
project_hash = hash(concatenate(sorted(file_hashes)))
```

If project_hash matches previous run:

* Skip recomputation (unless force flag)

Store in:

Project.last_hash

---

# 8Ô∏è‚É£ Migration Strategy

Version your schema.

Add table:

```id="jfi839"
schema_metadata (
  version TEXT,
  applied_at TIMESTAMP
)
```

If schema changes:

* Increment version
* Write migration SQL
* Never silently modify

This gives production-level discipline.

---

# 9Ô∏è‚É£ Storage Layout on Disk

Inside Docker:

```
/data/
   /runs/
      /{analysis_id}/
           graph.json
           summary.json
           logs.txt
database.sqlite
```

Never mix artifacts across runs.

---

# üîü Data Lifecycle Rules

When should data be deleted?

Add:

DELETE /analysis/{analysis_id}

Should:

* Delete DB rows
* Delete folder
* Maintain project record

---

# 11Ô∏è‚É£ Critical Design Strengths

This persistence model:

* Enables reproducibility
* Enables comparison across runs
* Enables performance benchmarking
* Enables academic validation
* Enables caching
* Prevents data corruption
* Keeps graph and relational concerns separated

Most student projects never reach this level of rigor.

---

# 12Ô∏è‚É£ Known Weaknesses

Be honest:

* SQLite limits concurrency (acceptable for local-first)
* Edge storage in JSON reduces SQL query flexibility
* Blast radius stored per run may increase DB size
* Large graphs increase disk footprint

All acceptable trade-offs.

---

# üöÄ Where You Stand Now

You now have:

1. Charter
2. Canonical AST schema
3. Graph model
4. Risk & extraction model
5. API specification
6. Persistence & ER design

At this point:

Your architecture is fully defined.

Now implementation can begin without structural ambiguity.

